{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87e79169",
   "metadata": {},
   "source": [
    "ollama-python SDK\n",
    "Repo: https://github.com/ollama/ollama-python/tree/main\n",
    "Examples: https://github.com/ollama/ollama-python/tree/main/examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2126d299",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import ollama\n",
    "from ollama import chat, generate\n",
    "from pprint import pprint\n",
    "from pydantic import BaseModel\n",
    "from ollama import ChatResponse, GenerateResponse, EmbedResponse, Client, AsyncClient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af15d731",
   "metadata": {},
   "source": [
    "## Single prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bc8b138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ciao Davide! It's nice to meet you. Is there something I can help you with or would you like to chat?\n"
     ]
    }
   ],
   "source": [
    "response: GenerateResponse = generate(model='llama3.2', prompt=\"Hello my name is Davide\")\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c14af7",
   "metadata": {},
   "source": [
    "## ChatResponse Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbaba593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['construct',\n",
      " 'context',\n",
      " 'copy',\n",
      " 'created_at',\n",
      " 'dict',\n",
      " 'done',\n",
      " 'done_reason',\n",
      " 'eval_count',\n",
      " 'eval_duration',\n",
      " 'from_orm',\n",
      " 'get',\n",
      " 'json',\n",
      " 'load_duration',\n",
      " 'model',\n",
      " 'model_computed_fields',\n",
      " 'model_config',\n",
      " 'model_construct',\n",
      " 'model_copy',\n",
      " 'model_dump',\n",
      " 'model_dump_json',\n",
      " 'model_extra',\n",
      " 'model_fields',\n",
      " 'model_fields_set',\n",
      " 'model_json_schema',\n",
      " 'model_parametrized_name',\n",
      " 'model_post_init',\n",
      " 'model_rebuild',\n",
      " 'model_validate',\n",
      " 'model_validate_json',\n",
      " 'model_validate_strings',\n",
      " 'parse_file',\n",
      " 'parse_obj',\n",
      " 'parse_raw',\n",
      " 'prompt_eval_count',\n",
      " 'prompt_eval_duration',\n",
      " 'response',\n",
      " 'schema',\n",
      " 'schema_json',\n",
      " 'thinking',\n",
      " 'total_duration',\n",
      " 'update_forward_refs',\n",
      " 'validate']\n"
     ]
    }
   ],
   "source": [
    "pprint([x for x in dir(response) if ((not x.startswith(\"_\")) and (not callable(x)))]) # response object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bc5959",
   "metadata": {},
   "source": [
    "## Conversation Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc6464eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#1:  \"Swish\" is a perfect rhyme for \"dish\".\n",
      "#2:  Here's a rhyme: I'm on a swish, got my heart in a single wish.\n"
     ]
    }
   ],
   "source": [
    "rhyme_word = \"dish\"\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a expert rapper. You can find rhymes with a lot of words and construct rap lyrics. Keep your answers limited to a single sentence\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"What rhymes with the word '{rhyme_word}'\"\n",
    "    },\n",
    "]\n",
    "\n",
    "response: ChatResponse = chat(model='llama3.2', messages=messages)\n",
    "print(\"#1: \", response['message']['content'])\n",
    "\n",
    "messages.append(response.message)\n",
    "messages.append({\"role\": \"user\", \"content\": \"Now make a rhyme with the word single and the previous word rhyme word.\"})\n",
    "response: ChatResponse = chat(model='llama3.2', messages=messages)\n",
    "print(\"#2: \", response['message']['content'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9593ca74",
   "metadata": {},
   "source": [
    "## Streaming Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8566752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sky appears blue because of a phenomenon called Rayleigh scattering, named after the British physicist Lord Rayleigh, who first described it in the late 19th century.\n",
      "\n",
      "Here's what happens:\n",
      "\n",
      "1. **Sunlight enters Earth's atmosphere**: When sunlight enters our atmosphere, it consists of a broad spectrum of colors, including all the colors of the visible light (red, orange, yellow, green, blue, indigo, and violet).\n",
      "2. **Light interacts with atmospheric molecules**: As sunlight travels through the atmosphere, it encounters tiny molecules of gases such as nitrogen (N2) and oxygen (O2). These molecules scatter the light in all directions.\n",
      "3. **Shorter wavelengths are scattered more**: The smaller wavelength colors (like blue and violet) are scattered more than the longer wavelength colors (like red and orange). This is because the smaller molecules are more effective at scattering shorter wavelengths.\n",
      "4. **Blue light is scattered in all directions**: As a result of Rayleigh scattering, the blue light is scattered in all directions by the atmospheric molecules. This means that the blue light is distributed evenly throughout the atmosphere, making it visible to our eyes from any direction.\n",
      "5. **Our eyes see the scattered blue light**: When we look up at the sky, our eyes see the scattered blue light that has been reflected off the tiny molecules in the atmosphere. This is why the sky appears blue during the daytime, when the sun is overhead.\n",
      "\n",
      "It's worth noting that:\n",
      "\n",
      "* The color of the sky can vary depending on the time of day and atmospheric conditions. For example, during sunrise and sunset, the sky often takes on hues of orange, pink, and red due to the scattering of longer wavelengths.\n",
      "* The sky appears more blue in areas with fewer air pollutants and less atmospheric haze, as these particles can scatter light in different ways.\n",
      "\n",
      "I hope that helps you understand why the sky is blue!"
     ]
    }
   ],
   "source": [
    "\n",
    "stream = chat(\n",
    "    model='llama3.2',\n",
    "    messages=[{'role': 'user', 'content': 'Why is the sky blue?'}],\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "  print(chunk['message']['content'], end='', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0219cf23",
   "metadata": {},
   "source": [
    "## Sampling Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "234d24de",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "dictionary update sequence element #0 has length 1; 2 is required",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m initial_message = {\u001b[33m'\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33muser\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mWhy is the sky blue?\u001b[39m\u001b[33m'\u001b[39m}\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m response: ChatResponse = \u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mllama3.2\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43minitial_message\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.9\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m#1: \u001b[39m\u001b[33m\"\u001b[39m, response[\u001b[33m'\u001b[39m\u001b[33mmessage\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m      5\u001b[39m response: ChatResponse = chat(model=\u001b[33m'\u001b[39m\u001b[33mllama3.2\u001b[39m\u001b[33m'\u001b[39m, messages=initial_message, options={\u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m0.7\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtop_p\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m0.9\u001b[39m})\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/code/python/ollama-agents/.venv/lib/python3.12/site-packages/ollama/_client.py:348\u001b[39m, in \u001b[36mClient.chat\u001b[39m\u001b[34m(self, model, messages, tools, stream, think, format, options, keep_alive)\u001b[39m\n\u001b[32m    297\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mchat\u001b[39m(\n\u001b[32m    298\u001b[39m   \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    299\u001b[39m   model: \u001b[38;5;28mstr\u001b[39m = \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    307\u001b[39m   keep_alive: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    308\u001b[39m ) -> Union[ChatResponse, Iterator[ChatResponse]]:\n\u001b[32m    309\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    310\u001b[39m \u001b[33;03m  Create a chat response using the requested model.\u001b[39;00m\n\u001b[32m    311\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    340\u001b[39m \u001b[33;03m  Returns `ChatResponse` if `stream` is `False`, otherwise returns a `ChatResponse` generator.\u001b[39;00m\n\u001b[32m    341\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m    342\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._request(\n\u001b[32m    343\u001b[39m     ChatResponse,\n\u001b[32m    344\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mPOST\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    345\u001b[39m     \u001b[33m'\u001b[39m\u001b[33m/api/chat\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    346\u001b[39m     json=ChatRequest(\n\u001b[32m    347\u001b[39m       model=model,\n\u001b[32m--> \u001b[39m\u001b[32m348\u001b[39m       messages=\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_copy_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    349\u001b[39m       tools=\u001b[38;5;28mlist\u001b[39m(_copy_tools(tools)),\n\u001b[32m    350\u001b[39m       stream=stream,\n\u001b[32m    351\u001b[39m       think=think,\n\u001b[32m    352\u001b[39m       \u001b[38;5;28mformat\u001b[39m=\u001b[38;5;28mformat\u001b[39m,\n\u001b[32m    353\u001b[39m       options=options,\n\u001b[32m    354\u001b[39m       keep_alive=keep_alive,\n\u001b[32m    355\u001b[39m     ).model_dump(exclude_none=\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[32m    356\u001b[39m     stream=stream,\n\u001b[32m    357\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/code/python/ollama-agents/.venv/lib/python3.12/site-packages/ollama/_client.py:1154\u001b[39m, in \u001b[36m_copy_messages\u001b[39m\u001b[34m(messages)\u001b[39m\n\u001b[32m   1151\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_copy_messages\u001b[39m(messages: Optional[Sequence[Union[Mapping[\u001b[38;5;28mstr\u001b[39m, Any], Message]]]) -> Iterator[Message]:\n\u001b[32m   1152\u001b[39m   \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m messages \u001b[38;5;129;01mor\u001b[39;00m []:\n\u001b[32m   1153\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m Message.model_validate(\n\u001b[32m-> \u001b[39m\u001b[32m1154\u001b[39m       {k: \u001b[38;5;28mlist\u001b[39m(_copy_images(v)) \u001b[38;5;28;01mif\u001b[39;00m k == \u001b[33m'\u001b[39m\u001b[33mimages\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m)\u001b[49m.items() \u001b[38;5;28;01mif\u001b[39;00m v},\n\u001b[32m   1155\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: dictionary update sequence element #0 has length 1; 2 is required"
     ]
    }
   ],
   "source": [
    "initial_message = {'role': 'user', 'content': 'Why is the sky blue?'}\n",
    "\n",
    "response: ChatResponse = chat(model='llama3.2', messages=initial_message, options={\"temperature\": 0.7, \"top_p\": 0.9})\n",
    "print(\"#1: \", response['message']['content'])\n",
    "response: ChatResponse = chat(model='llama3.2', messages=initial_message, options={\"temperature\": 0.7, \"top_p\": 0.9})\n",
    "print(\"#2: \", response['message']['content'])\n",
    "response: ChatResponse = chat(model='llama3.2', messages=initial_message, options={\"temperature\": 0.0, \"top_p\": 0.9})\n",
    "print(\"#3: \", response['message']['content'])\n",
    "response: ChatResponse = chat(model='llama3.2', messages=initial_message, options={\"temperature\": 0.0, \"top_p\": 0.9})\n",
    "print(\"#4: \", response['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fde6a1",
   "metadata": {},
   "source": [
    "## Custom Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e10c2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(\n",
    "  host='http://localhost:11434',\n",
    "  headers={'x-some-header': 'some-value'}\n",
    ")\n",
    "response = client.chat(model='llama3.2', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'Why is the sky blue?',\n",
    "  },\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40932905",
   "metadata": {},
   "source": [
    "## Async Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48e52f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sky appears blue because of a phenomenon called scattering, which occurs when sunlight interacts with the tiny molecules of gases in the Earth's atmosphere.\n",
      "\n",
      "Here's what happens:\n",
      "\n",
      "1. When sunlight enters the Earth's atmosphere, it encounters tiny molecules of nitrogen (N2) and oxygen (O2).\n",
      "2. These molecules scatter the light in all directions, but they scatter shorter (blue) wavelengths more than longer (red) wavelengths.\n",
      "3. This is known as Rayleigh scattering, named after the British physicist Lord Rayleigh, who first described it in the late 19th century.\n",
      "4. As a result of this scattering, the blue light is distributed throughout the atmosphere and reaches our eyes from all directions.\n",
      "5. Our eyes perceive this scattered blue light as the color of the sky.\n",
      "\n",
      "It's worth noting that the color of the sky can appear different under various conditions, such as:\n",
      "\n",
      "* During sunrise and sunset, when the light has to travel through more of the Earth's atmosphere, scattering occurs more intensely, making the sky appear redder.\n",
      "* At high altitudes or in areas with very few air molecules, the sky may appear more pale or grey.\n",
      "* On cloudy days, the scattered light is obscured by the clouds, so we see a different color.\n",
      "\n",
      "So, to summarize, the sky appears blue because of the scattering of sunlight by tiny molecules in the Earth's atmosphere."
     ]
    }
   ],
   "source": [
    "\n",
    "async def async_chat():\n",
    "  message = {'role': 'user', 'content': 'Why is the sky blue?'}\n",
    "  response = await AsyncClient().chat(model='llama3.2', messages=[message])\n",
    "\n",
    "\n",
    "await async_chat() # for notebook use this\n",
    "# asyncio.run(async_chat()) # for python mode use this\n",
    "\n",
    "async def async_chat_stream():\n",
    "  message = {'role': 'user', 'content': 'Why is the sky blue?'}\n",
    "  async for part in await AsyncClient().chat(model='llama3.2', messages=[message], stream=True):\n",
    "    print(part['message']['content'], end='', flush=True)\n",
    "\n",
    "await async_chat_stream() # for notebook use this\n",
    "# asyncio.run(async_chat_stream()) # for python mode use this"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ef31b7",
   "metadata": {},
   "source": [
    "# Tool Use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23a19af",
   "metadata": {},
   "source": [
    "## Single Tool Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "495ef3b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: What is three plus one?\n"
     ]
    },
    {
     "ename": "ResponseError",
     "evalue": "model \"llama3.1\" not found, try pulling it first (status code: 404)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mResponseError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 52\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mPrompt:\u001b[39m\u001b[33m'\u001b[39m, messages[\u001b[32m0\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     47\u001b[39m available_functions = {\n\u001b[32m     48\u001b[39m   \u001b[33m'\u001b[39m\u001b[33madd_two_numbers\u001b[39m\u001b[33m'\u001b[39m: add_two_numbers,\n\u001b[32m     49\u001b[39m   \u001b[33m'\u001b[39m\u001b[33msubtract_two_numbers\u001b[39m\u001b[33m'\u001b[39m: subtract_two_numbers,\n\u001b[32m     50\u001b[39m }\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m response: ChatResponse = \u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m  \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mllama3.1\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m  \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m  \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43madd_two_numbers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubtract_two_numbers_tool\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response.message.tool_calls:\n\u001b[32m     59\u001b[39m   \u001b[38;5;66;03m# There may be multiple tool calls in the response\u001b[39;00m\n\u001b[32m     60\u001b[39m   \u001b[38;5;28;01mfor\u001b[39;00m tool \u001b[38;5;129;01min\u001b[39;00m response.message.tool_calls:\n\u001b[32m     61\u001b[39m     \u001b[38;5;66;03m# Ensure the function is available, and then call it\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/code/python/ollama-agents/.venv/lib/python3.12/site-packages/ollama/_client.py:342\u001b[39m, in \u001b[36mClient.chat\u001b[39m\u001b[34m(self, model, messages, tools, stream, think, format, options, keep_alive)\u001b[39m\n\u001b[32m    297\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mchat\u001b[39m(\n\u001b[32m    298\u001b[39m   \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    299\u001b[39m   model: \u001b[38;5;28mstr\u001b[39m = \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    307\u001b[39m   keep_alive: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    308\u001b[39m ) -> Union[ChatResponse, Iterator[ChatResponse]]:\n\u001b[32m    309\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    310\u001b[39m \u001b[33;03m  Create a chat response using the requested model.\u001b[39;00m\n\u001b[32m    311\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    340\u001b[39m \u001b[33;03m  Returns `ChatResponse` if `stream` is `False`, otherwise returns a `ChatResponse` generator.\u001b[39;00m\n\u001b[32m    341\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m342\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    343\u001b[39m \u001b[43m    \u001b[49m\u001b[43mChatResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    344\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mPOST\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    345\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m/api/chat\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    346\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatRequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_copy_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m      \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_copy_tools\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtools\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m      \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m      \u001b[49m\u001b[43mthink\u001b[49m\u001b[43m=\u001b[49m\u001b[43mthink\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m      \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m      \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m      \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_dump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexclude_none\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/code/python/ollama-agents/.venv/lib/python3.12/site-packages/ollama/_client.py:180\u001b[39m, in \u001b[36mClient._request\u001b[39m\u001b[34m(self, cls, stream, *args, **kwargs)\u001b[39m\n\u001b[32m    176\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(**part)\n\u001b[32m    178\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(**\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request_raw\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m.json())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/code/python/ollama-agents/.venv/lib/python3.12/site-packages/ollama/_client.py:124\u001b[39m, in \u001b[36mClient._request_raw\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    122\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m r\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.HTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ResponseError(e.response.text, e.response.status_code) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.ConnectError:\n\u001b[32m    126\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(CONNECTION_ERROR_MESSAGE) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mResponseError\u001b[39m: model \"llama3.1\" not found, try pulling it first (status code: 404)"
     ]
    }
   ],
   "source": [
    "\n",
    "def add_two_numbers(a: int, b: int) -> int:\n",
    "  \"\"\"\n",
    "  Add two numbers\n",
    "\n",
    "  Args:\n",
    "    a (int): The first number\n",
    "    b (int): The second number\n",
    "\n",
    "  Returns:\n",
    "    int: The sum of the two numbers\n",
    "  \"\"\"\n",
    "\n",
    "  # The cast is necessary as returned tool call arguments don't always conform exactly to schema\n",
    "  # E.g. this would prevent \"what is 30 + 12\" to produce '3012' instead of 42\n",
    "  return int(a) + int(b)\n",
    "\n",
    "\n",
    "def subtract_two_numbers(a: int, b: int) -> int:\n",
    "  \"\"\"\n",
    "  Subtract two numbers\n",
    "  \"\"\"\n",
    "\n",
    "  # The cast is necessary as returned tool call arguments don't always conform exactly to schema\n",
    "  return int(a) - int(b)\n",
    "\n",
    "\n",
    "# Tools can still be manually defined and passed into chat\n",
    "subtract_two_numbers_tool = {\n",
    "  'type': 'function',\n",
    "  'function': {\n",
    "    'name': 'subtract_two_numbers',\n",
    "    'description': 'Subtract two numbers',\n",
    "    'parameters': {\n",
    "      'type': 'object',\n",
    "      'required': ['a', 'b'],\n",
    "      'properties': {\n",
    "        'a': {'type': 'integer', 'description': 'The first number'},\n",
    "        'b': {'type': 'integer', 'description': 'The second number'},\n",
    "      },\n",
    "    },\n",
    "  },\n",
    "}\n",
    "\n",
    "messages = [{'role': 'user', 'content': 'What is three plus one?'}]\n",
    "print('Prompt:', messages[0]['content'])\n",
    "\n",
    "available_functions = {\n",
    "  'add_two_numbers': add_two_numbers,\n",
    "  'subtract_two_numbers': subtract_two_numbers,\n",
    "}\n",
    "\n",
    "response: ChatResponse = chat(\n",
    "  'llama3.1',\n",
    "  messages=messages,\n",
    "  tools=[add_two_numbers, subtract_two_numbers_tool],\n",
    ")\n",
    "\n",
    "if response.message.tool_calls:\n",
    "  # There may be multiple tool calls in the response\n",
    "  for tool in response.message.tool_calls:\n",
    "    # Ensure the function is available, and then call it\n",
    "    if function_to_call := available_functions.get(tool.function.name):\n",
    "      print('Calling function:', tool.function.name)\n",
    "      print('Arguments:', tool.function.arguments)\n",
    "      output = function_to_call(**tool.function.arguments)\n",
    "      print('Function output:', output)\n",
    "    else:\n",
    "      print('Function', tool.function.name, 'not found')\n",
    "\n",
    "# Only needed to chat with the model using the tool call results\n",
    "if response.message.tool_calls:\n",
    "  # Add the function response to messages for the model to use\n",
    "  messages.append(response.message)\n",
    "  messages.append({'role': 'tool', 'content': str(output), 'tool_name': tool.function.name})\n",
    "\n",
    "  # Get final response from model with function outputs\n",
    "  final_response = chat('llama3.2', messages=messages)\n",
    "  print('Final response:', final_response.message.content)\n",
    "\n",
    "else:\n",
    "  print('No tool calls returned from model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad1a8fa",
   "metadata": {},
   "source": [
    "## Multi-tool Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b79508",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import Iterator\n",
    "\n",
    "\n",
    "def get_temperature(city: str) -> int:\n",
    "  \"\"\"\n",
    "  Get the temperature for a city in Celsius\n",
    "\n",
    "  Args:\n",
    "    city (str): The name of the city\n",
    "\n",
    "  Returns:\n",
    "    int: The current temperature in Celsius\n",
    "  \"\"\"\n",
    "  # This is a mock implementation - would need to use a real weather API\n",
    "  import random\n",
    "\n",
    "  if city not in ['London', 'Paris', 'New York', 'Tokyo', 'Sydney']:\n",
    "    return 'Unknown city'\n",
    "\n",
    "  return str(random.randint(0, 35)) + ' degrees Celsius'\n",
    "\n",
    "\n",
    "def get_conditions(city: str) -> str:\n",
    "  \"\"\"\n",
    "  Get the weather conditions for a city\n",
    "  \"\"\"\n",
    "  if city not in ['London', 'Paris', 'New York', 'Tokyo', 'Sydney']:\n",
    "    return 'Unknown city'\n",
    "  # This is a mock implementation - would need to use a real weather API\n",
    "  conditions = ['sunny', 'cloudy', 'rainy', 'snowy']\n",
    "  return random.choice(conditions)\n",
    "\n",
    "\n",
    "available_functions = {\n",
    "  'get_temperature': get_temperature,\n",
    "  'get_conditions': get_conditions,\n",
    "}\n",
    "\n",
    "\n",
    "cities = ['London', 'Paris', 'New York', 'Tokyo', 'Sydney']\n",
    "city = random.choice(cities)\n",
    "city2 = random.choice(cities)\n",
    "messages = [{'role': 'user', 'content': f'What is the temperature in {city}? and what are the weather conditions in {city2}?'}]\n",
    "print('----- Prompt:', messages[0]['content'], '\\n')\n",
    "\n",
    "model = 'qwen3'\n",
    "client = Client()\n",
    "response: Iterator[ChatResponse] = client.chat(model, stream=True, messages=messages, tools=[get_temperature, get_conditions], think=True)\n",
    "\n",
    "for chunk in response:\n",
    "  if chunk.message.thinking:\n",
    "    print(chunk.message.thinking, end='', flush=True)\n",
    "  if chunk.message.content:\n",
    "    print(chunk.message.content, end='', flush=True)\n",
    "  if chunk.message.tool_calls:\n",
    "    for tool in chunk.message.tool_calls:\n",
    "      if function_to_call := available_functions.get(tool.function.name):\n",
    "        print('\\nCalling function:', tool.function.name, 'with arguments:', tool.function.arguments)\n",
    "        output = function_to_call(**tool.function.arguments)\n",
    "        print('> Function output:', output, '\\n')\n",
    "\n",
    "        # Add the assistant message and tool call result to the messages\n",
    "        messages.append(chunk.message)\n",
    "        messages.append({'role': 'tool', 'content': str(output), 'tool_name': tool.function.name})\n",
    "      else:\n",
    "        print('Function', tool.function.name, 'not found')\n",
    "\n",
    "print('----- Sending result back to model \\n')\n",
    "if any(msg.get('role') == 'tool' for msg in messages):\n",
    "  res = client.chat(model, stream=True, tools=[get_temperature, get_conditions], messages=messages, think=True)\n",
    "  done_thinking = False\n",
    "  for chunk in res:\n",
    "    if chunk.message.thinking:\n",
    "      print(chunk.message.thinking, end='', flush=True)\n",
    "    if chunk.message.content:\n",
    "      if not done_thinking:\n",
    "        print('\\n----- Final result:')\n",
    "        done_thinking = True\n",
    "      print(chunk.message.content, end='', flush=True)\n",
    "    if chunk.message.tool_calls:\n",
    "      # Model should be explaining the tool calls and the results in this output\n",
    "      print('Model returned tool calls:')\n",
    "      print(chunk.message.tool_calls)\n",
    "else:\n",
    "  print('No tool calls returned')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7e0bae",
   "metadata": {},
   "source": [
    "## Structured Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64d45e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the schema for the response\n",
    "class FriendInfo(BaseModel):\n",
    "  name: str\n",
    "  age: int\n",
    "  is_available: bool\n",
    "\n",
    "\n",
    "class FriendList(BaseModel):\n",
    "  friends: list[FriendInfo]\n",
    "\n",
    "\n",
    "schema = {'type': 'object', 'properties': {'friends': {'type': 'array', 'items': {'type': 'object', 'properties': {'name': {'type': 'string'}, 'age': {'type': 'integer'}, 'is_available': {'type': 'boolean'}}, 'required': ['name', 'age', 'is_available']}}}, 'required': ['friends']}\n",
    "response = chat(\n",
    "  model='llama3.1:8b',\n",
    "  messages=[{'role': 'user', 'content': 'I have two friends. The first is Ollama 22 years old busy saving the world, and the second is Alonso 23 years old and wants to hang out. Return a list of friends in JSON format'}],\n",
    "  format=FriendList.model_json_schema(),  # Use Pydantic to generate the schema or format=schema\n",
    "  options={'temperature': 0},  # Make responses more deterministic\n",
    ")\n",
    "\n",
    "# Use Pydantic to validate the response\n",
    "friends_response = FriendList.model_validate_json(response.message.content)\n",
    "print(friends_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32610585",
   "metadata": {},
   "source": [
    "## Thinking / Reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea571beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'What is 10 + 23 * 2?',\n",
    "  },\n",
    "]\n",
    "\n",
    "response_think = chat(\"qwen3:1.7b\", messages=messages, think=True)\n",
    "response_nothink = chat(\"qwen3:1.7b\", messages=messages, think=False)\n",
    "\n",
    "print('Thinking:\\n========\\n\\n' + response_think.message.thinking)\n",
    "print('\\nResponse:\\n========\\n\\n' + response_think.message.content)\n",
    "print('\\n No Think Response:\\n========\\n\\n' + response_nothink.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7635cb1e",
   "metadata": {},
   "source": [
    "# Multi-modal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9afa4f",
   "metadata": {},
   "source": [
    "## Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbfc4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('player.png', 'rb') as file:\n",
    "  response = ollama.chat(\n",
    "    model='llama3.2-vision',\n",
    "    messages=[\n",
    "      {\n",
    "        'role': 'user',\n",
    "        'content': 'What color is the tie the man is wearing?',\n",
    "        'images': [file.read()],\n",
    "      },\n",
    "    ],\n",
    "  )\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86d5cc2",
   "metadata": {},
   "source": [
    "## Embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5c8970",
   "metadata": {},
   "outputs": [],
   "source": [
    "output: EmbedResponse = ollama.embed(model='llama3.1', input='The sky is blue because of rayleigh scattering')\n",
    "output_embeddings = output.embeddings\n",
    "\n",
    "print(output)\n",
    "print(\"Embedding dimension:\", len(output.embeddings[0]))\n",
    "\n",
    "\n",
    "batch_output: EmbedResponse = ollama.embed(model='llama3.1', input=['The sky is blue because of rayleigh scattering', 'Grass is green because of chlorophyll'])\n",
    "print(batch_output)\n",
    "print(\"Number of embeddings:\", len(batch_output.embeddings))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8212359",
   "metadata": {},
   "source": [
    "## API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daeb899f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ollama.list(), \"\\n####\\n\")\n",
    "print(ollama.show(\"llama3.2\"), \"\\n####\\n\")\n",
    "print(ollama.ps(), \"\\n####\\n\")\n",
    "\n",
    "\n",
    "# ollama.create(model='example', modelfile=modelfile) (using ModelFile)\n",
    "# ollama.create(model='example', from_='gemma3', system=\"You are Mario from Super Mario Bros.\")\n",
    "# ollama.copy('gemma3', 'user/gemma3')\n",
    "# ollama.delete('gemma3')\n",
    "# ollama.pull('gemma3')\n",
    "# ollama.push('user/gemma3')\n",
    "#\n",
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
